# -*- coding: utf-8 -*-
"""finetune_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fk4m0ulNxE-nZOmpu1eS6rGuHoGbkmL3
"""

!pip install peft
!pip install accelerate

!pip install transformers
!pip install datasets

!pip install bitsandBytes

!pip install GPUtil

import torch
import GPUtil as gutil
import os

gutil.showUtilization()

if torch.cuda.is_available():
  print("gpu")
else:
  print("cpu")

os.environ["CUDA_DEVICE_ORDER"]= "PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig , LlamaTokenizer
import transformers
from huggingface_hub import notebook_login
from peft import prepare_model_for_kbit_training , LoraConfig, get_peft_model

if "COLAB_GPU" in os.environ:
  from google.colab import output
  output.enable_custom_widget_manager()

if "COLAB_GPU" in os.environ:
  !huggingface-cli login
else:
  notebook_login()

base_model = "meta-llama/Llama-2-7b-chat-hf"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config
)

!git clone https://github.com/poloclub/Fine-tuning-LLMs.git

from datasets import load_dataset
train_dataset = load_dataset("text",data_files={"train":["/content/Fine-tuning-LLMs/data/hawaii_wf_1.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_2.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_3.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_4.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_5.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_6.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_7.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_8.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_9.txt", "/content/Fine-tuning-LLMs/data/hawaii_wf_10.txt"]},split="train")

train_dataset["text"][100]

tokenizer = LlamaTokenizer.from_pretrained(base_model, use_fast =False,trust_remote_code=True,add_eos_token=True)
if tokenizer.pad_token is None:
  tokenizer.add_special_tokens({'pad_token':tokenizer.eos_token})

tokenized_dataset = []
for line in train_dataset:
  tokenized_dataset.append(tokenizer(line["text"]))

tokenized_dataset[1]

tokenizer.eos_token

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

config = LoraConfig(
    r=8,
    lora_alpha=64,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    bias="none",
    lora_dropout=0.05,
    task_type="CAUSAL_LM"
)

model = get_peft_model(model,config)

trainer = transformers.Trainer(
    model = model,
    train_dataset = tokenized_dataset,
    args =  transformers.TrainingArguments(
        output_dir = "/finetunedModel",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=2,
        num_train_epochs=3,
        learning_rate=1e-4,
        max_steps=20,
        bf16=False,
        optim="paged_adamw_8bit",
        logging_dir="./log",
        save_strategy="epoch",
        save_steps=50,
        logging_steps=10
    ),
    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
model.config.use_cache = False
trainer.train()

import torch
from transformers import AutoTokenizer,AutoModelForCausalLM
from transformers import BitsAndBytesConfig, LlamaTokenizer
from peft import PeftModel

base_model = "meta-llama/Llama-2-7b-chat-hf"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = LlamaTokenizer.from_pretrained(base_model, use_fast =False,trust_remote_code=True,add_eos_token=True)
model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    use_auth_token = True,
    trust_remote_code = True,
    device_map = "auto"
)

model_fine = PeftModel.from_pretrained(model,"/finetunedModel/checkpoint-20")

user = "when did the hawaii wildfires start?"
eval_prompt = f"Question: {user} Just answer this question accurately and concisely"


promptTokenized = tokenizer(eval_prompt, return_tensors="pt").to("cuda")

model_fine.eval()

with torch.no_grad():
    output = model_fine.generate(**promptTokenized, max_new_tokens=1024)
    print(tokenizer.decode(output[0], skip_special_tokens=True))

prompt = "The 2023 Hawaii wildfires were "
input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to("cuda")
output = model_fine.generate(input_ids=input_ids, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)
print(tokenizer.decode(output[0], skip_special_tokens=True))